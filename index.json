[{"content":"Natural Selection is what allows our species (and images!) to improve over time. In this article we\u0026rsquo;ll implement a scoring mechanism, through which \u0026ldquo;Mona Lisa\u0026rdquo; will actually look like one.\nNatural Selection Natural selection is the process though which species adapt to their environments. If the evolution is a wheel, then natural selection is the force that spins it. Organism that are better adapted tend to produce more offspring and pass on their genes. This process favours genes that aided their bearers to survive/reproduce, increasing their number in the following generations.\nIn biology \u0026ldquo;fitness\u0026rdquo; is defined by how successful an organism is at reproduction.\nWikipedia says:\nIf an organism lives half as long as others of its species, but has twice as many offspring surviving to adulthood, its genes become more common in the adult population of the next generation.\n\u0026hellip; and also \u0026hellip;\nIt is also equal to the average contribution to the gene pool of the next generation, made by the same individuals of the specified genotype or phenotype.\nWe, however, will define \u0026ldquo;fitness\u0026rdquo; as a difference between an organism and the ideal. Which is a bit vague, as there\u0026rsquo;s no obvious way of substituting one image from another and produce an integer. We\u0026rsquo;ll get back to that in a bit.\nLoss functions Genetic Algorithms (and Evolutionary Algorithms) are optimization algorithms that need a \u0026ldquo;goodness\u0026rdquo; of an organism, in order to decide whether to discard it. We\u0026rsquo;re going to implement two scoring methods, both based on loss functions. L1 Loss Function and L2 Loss Function are defined as follows:\n$$ L1 = \\sum_{i=0}^n \\vert y_{true_i} - y_{predicted_i} \\vert \\newline L2 = \\sum_{i=0}^n \\left( y_{true_i} - y_{predicted_i} \\right)^2 $$\nn represents the size of the ideal image in pixels; we know, that both images have the exact same size, so it will never out-of-range.\nBoth of these functions are used to covert an \u0026ldquo;object\u0026rdquo; or an \u0026ldquo;event\u0026rdquo;, to a real number representing its score. Which one should be picked then? In general L2 Loss Function is preferred in most of the cases. However, when the dataset has outliers, then L2 Loss Function does not perform well \u0026ndash; it leads to much larger errors.\nCool, we have a way of calculating differences between images\u0026rsquo; pixels. But how to calculate a difference between two pixels? That question was already answered in Art From Chaos. We take each of the pixels color channels and calculate their differences.\n$$ f(O, S) = \\sum_{i=0}^n \\vert (r_2 - r_1)^2 + (g_2 - g_1)^2 + (b_2 - b_1)^2 \\vert $$\nScoring mechanism First, let\u0026rsquo;s define a trait whereby the rest of the system will be able to interact with scoring methods.\npub trait FitnessFunction { /// This method calculates the fitness of `second_image` relative to `first_image`. /// /// In other words, it returns a value describing difference between those two images. The higher the value, the /// more those images are different from each other. fn calculate_fitness(\u0026amp;self, first_image: \u0026amp;Image, second_image: \u0026amp;Image) -\u0026gt; usize; } The method assumes that the first_image is the one being scored and second_image is the ideal. However, what\u0026rsquo;s nice about these loss functions, is that they return absolute values \u0026ndash; it does not matter which parameter is the ideal.\n// True for both L1 and L2 assert_eq!( scorer.calculate_fitness(first_image, second_image), scorer.calculate_fitness(second_image, first_image) ); Implementation of L1: AbsoluteDistance The implementation isn\u0026rsquo;t complex - first we fold each pixel pair to a usize, and then we sum those parts together to produce the score. Actually, we can do both by using Iterator\u0026rsquo;s fold method.\nfn fold_pixels(mut sum: usize, (p1, p2): (\u0026amp;Pixel, \u0026amp;Pixel)) -\u0026gt; usize { let diff_r = isize::from(p1.get_r()) - isize::from(p2.get_r()); let diff_g = isize::from(p1.get_g()) - isize::from(p2.get_g()); let diff_b = isize::from(p1.get_b()) - isize::from(p2.get_b()); sum += diff_r.unsigned_abs(); sum += diff_g.unsigned_abs(); sum += diff_b.unsigned_abs(); sum } #[derive(Debug, Default)] pub struct AbsoluteDistance; impl FitnessFunction for AbsoluteDistance { fn calculate_fitness( \u0026amp;self, first_image: \u0026amp;crate::models::Image, second_image: \u0026amp;crate::models::Image, ) -\u0026gt; usize { first_image .pixels() .iter() .zip(second_image.pixels().iter()) .fold(0usize, fold_pixels) } } Implementation of L2: SquareDistance Implementation of SquareDistance is almost identical. The only difference is the squaring of color channels.\nfn fold_pixels(mut sum: usize, (p1, p2): (\u0026amp;Pixel, \u0026amp;Pixel)) -\u0026gt; usize { let diff_r = isize::from(p1.get_r()) - isize::from(p2.get_r()); let diff_g = isize::from(p1.get_g()) - isize::from(p2.get_g()); let diff_b = isize::from(p1.get_b()) - isize::from(p2.get_b()); sum += diff_r.pow(2) as usize; sum += diff_g.pow(2) as usize; sum += diff_b.pow(2) as usize; sum } #[derive(Debug, Default)] pub struct SquareDistance; impl FitnessFunction for SquareDistance { fn calculate_fitness( \u0026amp;self, first_image: \u0026amp;crate::models::Image, second_image: \u0026amp;crate::models::Image, ) -\u0026gt; usize { first_image .pixels() .iter() .zip(second_image.pixels().iter()) .fold(0usize, fold_pixels) } } Fixing \u0026ldquo;Mona Lisa\u0026rdquo; Let\u0026rsquo;s wire up the scoring component with generation\u0026rsquo;s flow. During each generation, every specimens will be scored relatively to the ideal image. Using the scores we will select the best 5 1 organisms and discard the rest.\nSince there\u0026rsquo;s no crossover functionality yet, we need to fill the emptied generation space somehow. Simplest solution: once we have 5 best specimens, we\u0026rsquo;ll copy them over multiple times, to get a new generation 100 strong.\nMona Lisa (generation #10 000) 🤌\nThe image does look recognizable. And it\u0026rsquo;s still missing the last part \u0026ndash; crossing. However, as it\u0026rsquo;s not \u0026ldquo;required\u0026rdquo;, the algorithm works and it produces acceptable results.\nNext, we\u0026rsquo;ll implement the crossing function and we\u0026rsquo;ll see how much it improves algorithm\u0026rsquo;s efficiency \u0026ndash; defined as the derivative of specimen score with respect to generation number.\nPhoto by Fernando Venzano on Unsplash\nWhy 5? No particular reason. There should be enough specimen to fill the generation space again by combining them in varied ways. It could be more than 5, but we need to remember, that the goal of dropping those \u0026ldquo;bad\u0026rdquo; images is to discard mutations that resulted in decreasing overall \u0026ldquo;goodness\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://madebyme.today/articles/franklin/natural-selection-and-loss-functions/","summary":"Natural Selection is what allows our species (and images!) to improve over time. In this article we\u0026rsquo;ll implement a scoring mechanism, through which \u0026ldquo;Mona Lisa\u0026rdquo; will actually look like one.\n","title":"Natural Selection and Loss Functions"},{"content":"Today I was again setting up OpenPGP application on a new Yubikey. After over two years I already forgot how tedious that can be\u0026hellip; I\u0026rsquo;m writing this blog post to create a clear trace of what I needed to do today and hopefully, when the time comes to set up an another key, it\u0026rsquo;ll be as easy as opening up a blog entry.\nIn general I started to learn about security keys a few years back. I read \u0026ldquo;Security Keys: Practical Cryptographic Second Factors for the Modern Web\u0026rdquo; research paper by Google, explaining how they work and how they made them \u0026ldquo;fool-proof\u0026rdquo;. The article is very detailed and yet written in a easy-to-follow way. If you\u0026rsquo;re interested in security, then give it a try!\nThe rest of this article is a ramp about configuring a new key. If you don\u0026rsquo;t have one/not having issues with one at the moment, you can give it amiss.\nSetting up a new key Yubico offers great software for managing your keys. If you\u0026rsquo;re planning on using yours as a 2FA method or FIDO2, then you are a happier person.\nYubikey \u0026amp; macOS To start with, I could not make the key detectable by GnuPG. I was getting \u0026ldquo;Operation not supported by device\u0026rdquo; error.\ngpg --card-status gpg: selecting card failed: Operation not supported by device gpg: OpenPGP card not available: Operation not supported by device Then I found DataDog\u0026rsquo;s Yubikey troubleshooting guide. It fixed my problem \u0026ndash; a config file was missing in my GnuPG home.\nYubikey \u0026amp; OpenPGP Working with gpg sucks.\nThe tool is very advanced and offering a lot of features, so naturally its CLI is complex, to say the least. As it was before, so it was now, I was saved by this marvelous guide on how to prepare PGP keys for a Yubikey.\nIt encompasses everything: from generating keys, to rotating keys. It also offers different solutions depending on how much you care about security. I highly encourage you to configure your key with this guide.\nIt got me through most of the OpenPGP stuff pretty smoothly. Again, I was lost on making the key work with gpg-agent.\nYubikey \u0026amp; SSH Once all three keys (Signing, Encryption, Authentication) are correctly set up, it\u0026rsquo;s the moment for the agent. I always miss two parts: configuring gpg-agent and enabling SSH for gpg-agent.\nThe first one requires these few lines at the end of your rc file. I use Z shell, so it\u0026rsquo;s .zshrc for me.\nEnabling SSH for gpg-agent can be done by adding gpg-agent.conf file to your GnuPG home.\nYou\u0026rsquo;ll probably need to update the path pointing to pinentry-program. Just run\nwhich pinentry-mac If you don\u0026rsquo;t have pinentry-mac installed, then\nbrew install pinentry Cool. The last and yet very important step is to relaunch the agent:\ngpgconf --kill gpg-agent It should hopefully work.\nTips To get public SSH key run: ssh-add -L. It was around that time, when I tried to fetch my remote repo from GitHub\u0026hellip; and it wasn\u0026rsquo;t working. I lost a significant amount of time looking for some configuration issue, but there wasn\u0026rsquo;t any. I redid everything again to no avail.\nThen I saw that some of my GitHub Actions are not being triggered. 🤔 I dig a bit more and it turned out GitHub was having issues.\nGitHub Status is a great site to keep in your RSS feed. Then issues arise, they cascade.\n","permalink":"https://madebyme.today/articles/how-to-security-keys/","summary":"Today I was again setting up OpenPGP application on a new Yubikey. After over two years I already forgot how tedious that can be\u0026hellip; I\u0026rsquo;m writing this blog post to create a clear trace of what I needed to do today and hopefully, when the time comes to set up an another key, it\u0026rsquo;ll be as easy as opening up a blog entry.\n","title":"How to: Security Keys"},{"content":"In the previous article from this series, we\u0026rsquo;ve talked about genetic algorithms and how they can be used to generate art. Now let\u0026rsquo;s put those ideas into action and focus on implementing the first part of our artistic toolset: mutators.\nPreparing the ecosystem Before we begin working on mutators, we need to prepare an environment in which our specimens can thrive. We don\u0026rsquo;t need much - right now the only thing is a generation. For the record, in this context, a generation is a collection of specimen which can be mutated, scored, and bred (basically experimented upon) to get us closer to the optimum. Since our specimens are images, they can be represented by the following structs:\nstruct Pixel { r: u8, g: u8, b: u8, } struct Image { height: usize, width: usize, pixels: Vec\u0026lt;Pixel\u0026gt;, } Representing each pixel as a 24-bit value gives us some flexibility here \u0026ndash; it allows us to operate on two color depths: true color (which uses 24-bit colors) and grayscale (8-bit). Creating a grayscale pixel can be done by setting up all color channels to the same value. True, it uses thrice as much memory as it could, but\u0026hellip;\nPremature optimization is the root of all evil.\n~ Donald Knuth\nMethod of initializing 1 the generation will affect how fast we can search the solution space. As the algorithm produces more fitted images, the specimens get closed to the source image. But we don\u0026rsquo;t really care about how fast the optimum can be achieved, frankly, we don\u0026rsquo;t really care about achieving the optimum in the first place. It is the process of getting more fitted images and seeing how they evolve what\u0026rsquo;s really interesting. Therefore our generation will be initialized by blank images - images filled by white pixels. It will reduce pace of solution space search, but will produce images that are more visually interesting. We are here to do art, after all. 🎨\nimpl Pixel { #[must_use] pub const fn white() -\u0026gt; Self { Pixel::new(255, 255, 255) } #[must_use] pub const fn new(r: u8, g: u8, b: u8) -\u0026gt; Self { Pixel { r, g, b } } } impl Image { #[must_use] pub fn new(height: usize, width: usize, pixels: Vec\u0026lt;Pixel\u0026gt;) -\u0026gt; Self { Self { height, width, pixels, } } #[must_use] pub fn blank(height: usize, width: usize, pixel: \u0026amp;Pixel) -\u0026gt; Self { let size = height * width; let pixels = vec![pixel.clone(); size]; Self::new(height, width, pixels) } } #[must_use] fn get_first_generation( vec_len: usize, image_height: usize, image_width: usize ) -\u0026gt; Vec\u0026lt;Image\u0026gt; { let pixel = Pixel::white(); vec![Image::blank(height, width, \u0026amp;pixel); vec_len] } This code takes care of initializing the generation. 👌\nThrowing dice and hoping for the best As was mentioned in the previous article, mutators act only on one specimen at a time, inserting random modification onto it. With that description alone, we can already define a contract for all mutators we\u0026rsquo;re going to implement:\npub trait Mutator { fn mutate(\u0026amp;self, image: \u0026amp;mut Image); } Why \u0026amp;self and not \u0026amp;mut self? Due to the fact that mutations are independent of one another, they can be performed concurrently. In fact, as we will see in the future articles, mutating and scoring are the only steps that can be easily run in parallel.\nA diagram showing the flow of actions applied on a single generation.\nThrowing a rectangular dice Rectangles are the easiest shape to draw both algorithmically and IRL; our first mutator will use rectangles as a mutation primitive. To generate a random rectangle we need have the following:\ncoordinates of one of its corners, width, height, fill color. Fill color is pretty straightforward, but other values have some constraints they need to meet. An image we\u0026rsquo;ll be mutating has width and height \u0026ndash; let\u0026rsquo;s assume it\u0026rsquo;s n and m respectively. Coordinates of one of the corners, in our case it\u0026rsquo;s going to be top-left, are limited by the image dimensions. Width and height are limited by both image dimensions, and the coordinates we just generated.\n$$ x \\in \\lbrack 0 .. n \\lbrack \\newline y \\in \\lbrack 0 .. m \\lbrack \\newline width \\in \\lbrack 1 .. n - x + 1 \\lbrack \\newline height \\in \\lbrack 1 .. m - y + 1 \\lbrack \\newline $$\nWhy coordinates intervals are right-open? Because if the mutator selects the very right or bottom edge, then the rectangle would need to have zero width/height. By not right-closing the intervals, we ensure that there\u0026rsquo;s at least one pixel which can be mutated 👌. Similarly both width and height intervals are right-open to ensure that the rectangle will not overflow the image.\nstruct RandomRectangle { x: usize, y: usize, width: usize, height: usize, } #[must_use] fn get_random_rectangle(random: \u0026amp;mut Random, image: \u0026amp;Image) -\u0026gt; RandomRectangle { let image_width = image.width(); let image_height = image.height(); let x = random.get_random(0usize, image_width); let y = random.get_random(0usize, image_height); let width = random.get_random(0usize, image_width - x) + 1; let height = random.get_random(0usize, image_height - y) + 1; RandomRectangle { x, y, width, height, } } Function get_random_rectangle is a neat helper: based on the given RNG 2 and image, it returns a struct representing a random rectangle within the boundaries of the image.\nOnly two things left to do: generate random color and draw the shape. The implementation of rectangle mutator will look like this:\n#[derive(Debug, Default)] pub struct RectangleMutator; impl Mutator for RectangleMutator { fn mutate(\u0026amp;self, image: \u0026amp;mut Image) { let mut random = Random::default(); let rect = get_random_rectangle(\u0026amp;mut random, image); let r = random.get_random(0u8, 255); let g = random.get_random(0u8, 255); let b = random.get_random(0u8, 255); let image_width = image.width(); for j in rect.y..(rect.height + rect.y) { for i in rect.x..(rect.width + rect.x) { let pixel = \u0026amp;mut image[j * image_width + i]; pixel.r(r); pixel.g(g); pixel.b(b); } } } } The implementation is really simple. First we get a random rectangle (using the helper function), then generate color channels, and finally draw the shape. Drawing itself is done by two nested loops iterating over columns and rows of the image. With the two indexes it references a specific pixel: j * image_width + 1 and finally overrides it using the color channels.\nCool, let\u0026rsquo;s see what the program generates after 10 000 generations when initialized with \u0026ldquo;Mona Lisa\u0026rdquo;.\nMona Lisa (generation #10 000) Doesn\u0026rsquo;t really looks like anything. 😐\nWhich isn\u0026rsquo;t very surprising; the code did what it was suppose to do: it generated random rectangles on the white image. Since we don\u0026rsquo;t have any scoring logic yet (that\u0026rsquo;s a topic for another article) the resulting image is composed of random noise. We\u0026rsquo;ll need to wait a bit longer to get an image that even remotely reflects \u0026ldquo;Mona Lisa\u0026rdquo;.\nThrowing dice of other shapes It\u0026rsquo;d be nice to have mutators other than RectangleMutator, which are able to mutate images with different shapes, but I\u0026rsquo;m not going to cover them here. The reason is simple \u0026ndash; they operate under the same rules: you need to define boundaries first and then you need to draw the desired shape. I\u0026rsquo;ve implemented two other mutators: TriangleMutator and CircleMutator. Their sources can be found here.\nAfterword You might\u0026rsquo;ve noticed that the code examples of this article are not strictly bounded together, meaning you cannot just copy them to have a working example. A bunch of things like: Random implementation, loading the original image, mutation loop, and the whole impl Image block are missing. If you want to have a working solution it\u0026rsquo;s here (locked down to the newest commit at the moment of writing \u0026ndash; 73aa8da). The goal of this series is not to go through every single line of code to build a working utility, but rather to present an idea. So, moving forward all future articles from this series will also be done in that style.\nPhoto by Alexander Grey on Unsplash\nStay tuned 🌊\nUsually the generation is generated randomly. See here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRandom is a project-private utility class. Source can be found here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://madebyme.today/articles/franklin/randomness-of-life/","summary":"In the previous article from this series, we\u0026rsquo;ve talked about genetic algorithms and how they can be used to generate art. Now let\u0026rsquo;s put those ideas into action and focus on implementing the first part of our artistic toolset: mutators.\n","title":"Randomness of Life"},{"content":"Making art is hard. Drawing pictures is tedious. With programming, however, we can automate things. The point of automation is to reduce the amount of manual labor. So let\u0026rsquo;s mix evolution, DNA, and programming together to make art that makes itself. Pictures that draw themselves.\nConcepts and definitions Before we dive deep into making art, we need to understand some important concepts \u0026ndash; first things first. Automatic art 1, at its core, uses generic algorithms. Wikipedia has a really nice page about them, if you\u0026rsquo;d like to read it. However for the sake of the article this is enough:\nA Genetic Algorithm is an algorithm inspired by the process of natural selection used to find solutions for optimization problems. It has three main parts:\nMutation - during which specimens are randomly changed, Scoring - during which specimens are ranked by their \u0026ldquo;ability to adapt to their environment\u0026rdquo;, Crossing - during which one or more specimens are mixed together to produce a new member. OK. 👌\nWith these terms out of the way, let\u0026rsquo;s try to understand how exactly it works. Imagine we need to find a solution for a problem. It can be anything, like finding optimal timetable for a university class. Firstly, we need to encode a solution as a series of bytes (their Genetic representation, if you will). Once we have that we can clone the encoded representation to create a generation.\nEach member of the generation will be randomly mutated and then scored based on how well they fit in our constrains set. For example: we might want to have a timetable which leaves just enough break time to eat a quick lunch, but not too much, so that we can go home earlier. This (and similar constrains) might be used to evaluate the value of our new mutated timetable. With each specimen evaluated we leave a few of the best and discard the rest 💀.\nThe last step is to mix our special timetables to create new ones to fill the generation again.\nTo mutate, to score, to kill, to breed, to continue the cycle of life. It might sound simple, but in reality it is shockingly efficient in searching the solution space.\nArt from evolution Alright. Now with the theory out of the way, let\u0026rsquo;s conceptualize a program for generating automatic art. Probably it\u0026rsquo;s a good moment to explain the clickbaity summary at the top of this article: the pictures will not draw themselves, the goal is to make a program which will generate art automatically. It\u0026rsquo;s going to be an iterative solution where each cycle is parametrized by the results of its predecessor.\nStep 0: Initialization Before we do anything towards evolution we need to prepare a generation first. So what\u0026rsquo;s our generation size? Let\u0026rsquo;s see if Wikipedia has something insightful to say:\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions.\n~ Genetic algorithm @ Wikipedia\nWell, that\u0026rsquo;s really not the most helpful answer. 😐\nIt really boils down to this: the more specimen we have, the more memory the program requires and more CPU time to process each generation. On the other hand, the more specimen a generation have, the wider portion of solution space it can search. Whatever the generation size will be, we need to have a prototype - a specimen, which cloned will fill the generation. Since we\u0026rsquo;re dealing with art here, a tabula rasa should be a fitting choice.\nStep 1: Mutation Mutation method greatly affects the end result, so it\u0026rsquo;s imperative to select a right one. Mutation algorithms are usually stateless, meaning modification of each specimen does not affect any other. In each iteration the mutator will introduce a small change into specimens\u0026rsquo; genetic representation. On images it can be, for example: changing random pixels. This works, but the final image looks too detailed (in a bad way).\nNow, the changes does not technically need to be small per se, however applying too big changes might result in overriding a portion of the genome that was making this particular specimen a good candidate, thus resulting in loosing progress achieved by previous generations.\nIf you\u0026rsquo;d google \u0026ldquo;generating images with genetic algorithm\u0026rdquo;, you\u0026rsquo;d find that most projects on the subject use geometric shapes when applying mutation. Simple onces, like circles, rectangles, and triangles are a good choice. From these it\u0026rsquo;s rectangles, that can be the most easily represented in code. Having said that let\u0026rsquo;s see how a single specimen might change over a few first iterations.\nAn exemplar of a 6-generation mutation process. Starting with generation 0 - no mutation applied, till generation 5 - five mutations applied.\nEach of the rectangles on that picture represents a random mutation applied onto the image, meaning that all information needed to unambiguously identify a rectangle (width, height, coordinates of one of the corners, and its color) have been randomly generated.\nThe above illustrates the risk of allowing mutations which are not constrained by their impact: the mutation introduced in 1st step has been almost fully overwritten after 5 mutations. Don\u0026rsquo;t get me wrong, the result might be beneficial, but overall we want to utilize genome that have evolved in past generations, not to discard it completely.\nStep 2: Scoring Implementing scoring function can be tricky. Basically we need to have a way of mapping each specimen into an integer value. Then with values for all specimens we can calculate a threshold and filter out all images above it. The genetic algorithm does not provide to us any way of determining whether a mutation has been beneficial; that is strictly depends on the implementation. So let\u0026rsquo;s talk about what it is exactly we\u0026rsquo;d like to achieve here.\nThe idea behind generating images through evolution is that we have an ideal to which we\u0026rsquo;re aiming to get as close as possible. An original image, from which will derive a collection of images similar to it, each mutated and scored multiple times. A scoring function could calculate a difference between the original image and the one being currently scored:\n$$ f(O, S) = \\sum_{i=0}^n | O_n - S_n | \\tag{1} $$\nBoth O and S refer to a collection of pixels representing the original image and the current specimen respectively, thus allowing us to index their pixels and calculate a difference between them. This, on its own, isn\u0026rsquo;t the most helpful piece of advice, as it glides over the fact that we a calculating a difference of pixels not numbers, we cannot do arithmetics on them. To fix that we need to be a bit more clever here.\nWe can utilize the fact that pixels are just color, usually represented in RGB notation. Each color in the RGB color space is represented by three numbers from 0 to 255 (each encoding the amount of red, green, and blue). Numbers on their own don\u0026rsquo;t have any meaning, its the context that makes them colors, points, or geometric shapes. If we\u0026rsquo;d interpret these three numbers as coordinates in three-dimensional space, then they would become points. In that case, the difference between two points can be implemented as the distance between then:\n$$ d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2} $$\nAlright 👌. The final thought: this formula calculates the difference between two points in space, but we don\u0026rsquo;t really need the distance, just an indication of how similar the two pixels are. Since calculating a square root on computers is expensive, we can remove that bit and we\u0026rsquo;re left with:\n$$ g(A, B) = (x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2 \\tag{2} $$\nBy combining (1) and (2) together we get:\n$$ f(O, S) = \\sum_{i=0}^n | (r_2 - r_1)^2 + (g_2 - g_1)^2 + (b_2 - b_1)^2 | $$\nThat was a bit more mathsy that I\u0026rsquo;ve initially anticipated ◕_◕.\nStep 3: Crossing In the last step the algorithm has to fill up almost emptied generation. This step, quoting Wikipedia, it\u0026rsquo;s:\n[\u0026hellip;] a genetic operator used to combine the genetic information of two parents to generate new offspring. [\u0026hellip;] Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction.\n~ Crossover (genetic algorithm)\nThere are several ways we can make it work, from naive ones:\ncreate an exact copy of one of the remaining images, create a mutated copy of one of the remaining images, split image into two halves and fill it with a respective half from one of the two parents, to more sophisticated ones:\nfor each pixel pair taken from two parents calculate an arithmetic average and use it to construct a new pixel, for each pixel pair taken from two parents calculate an weighted average and use it to construct a new pixel. The methods mentioned above differ in their complexity, but more importantly, in how fitting specimens they create. It\u0026rsquo;s worth to mention that in opposition to the previous two steps, this one isn\u0026rsquo;t strictly mandatory. The algorithm will still work without it and the generated images will look acceptable. With crossing, however the algorithm generates more fitting specimens, relative to a one without the 3rd step, in the same number of generations.\nWe will take a look at several of crossing methods mentioned above and we will plot scores of their specimens as a function of generation number, to see how well they perform. 📈\nNext steps This is the first article from a series about generating art through genetic algorithms. In the next articles (coming up soon-ish) we\u0026rsquo;ll turn those ideas into Rust code and after that we\u0026rsquo;ll finally make art that makes itself.\nPhoto by Brett Jordan on Unsplash\nSee you around!\n🌊\nTerm coined by me. If you want to read more about art generated by algorithms you should probably look for Algorithmic art.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://madebyme.today/articles/franklin/art-from-chaos/","summary":"Making art is hard. Drawing pictures is tedious. With programming, however, we can automate things. The point of automation is to reduce the amount of manual labor. So let\u0026rsquo;s mix evolution, DNA, and programming together to make art that makes itself. Pictures that draw themselves.\n","title":"Art From Chaos"},{"content":"Well it took some time (almost 6 years) to create a personal blog, but here we are.\nHow did we come here? This is not the first attempt on creating my own website. On my GitHub there are many (most of them private) repos, which contains some sort of personal website. All of them abandoned, but not this one!\n\u0026hellip; at least not yet. ✌(-‿-)✌ However I\u0026rsquo;m optimistic.\nTo better understand why I have such high hopes for this project let\u0026rsquo;s go down the rabbit hole and analyze its ancestors and try to point out why they failed.\nPlatform 1: 0x52 (Django 2.2) Django is one of the first tools I\u0026rsquo;ve ever used to create something on the web.\nDjango is a high-level Python web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n~ Django website\nUsing that framework I\u0026rsquo;ve built a web journal. The idea was that at any moment I could use one of my devices to create a new journal entry. They were automatically sorted by creation date and tagged with tokens retrieved from the entry\u0026rsquo;s content. The latter deserves a bit more digging into, so let\u0026rsquo;s consider the following entry:\nLorem ipsum @dolor sit amet, consectetur adipiscing elit. Aliquam sed eleifend magna. @Quisque venenatis ex ex, a suscipit purus iaculis ac. Sed @lacinia tincidunt nunc vitae consectetur.\nA tag is a sequence of characters between '@' and one of ',\u0026lt;.\u0026gt;/?;:\\'\u0026quot;[{]}\\\\|()=+#$%^\u0026amp;*~\\r\\n '. When an entry was either created or modified, then the logic extracted tags from content:\n@staticmethod def extract_tag_names(text): words = re.split(Tag.escape_delimiters(Tag.TAG_DELIMITERS), text) words = list(filter(None, words)) return [tag_name[1:].lower() for tag_name in words if tag_name.startswith(Tag.TAG_SYMBOL)] It worked pretty well. I could create a new tag or use an already existing one. When the tag was orphaned (meaning it was referenced by no entry) the logic was able to take care of that too. 👀\nThe solution was designed to be used by more that one user: each person would have an account and they would be able to access only their own entries and tags.\nWhat happened with that project? I used it for a while, but after some time I wasn\u0026rsquo;t really actively adding new entries, so it just fated away. Also it was more of a personal utility website, than a blog. It was publicly available on Heroku until quite recently actually. I took down the website when Heroku announced their removal of free product plans.\nPlatform 2: Titan (ASP.NET) This was an another iteration of personal website development. I\u0026rsquo;ve decided to use .NET for this one, because earlier in that year I got my first job in the field and I was hyped to build something with the technology we used at work (we were developing a few solutions and one of them was built on top of .NET Framework).\nSadly I deleted the source code some time ago as a part of my GitHub purge, but I remember quite vividly the problem(s) with this one. In a sentence: it was too overengineered.\nthe strategy is definitely: first make it work, then make it right, and, finally, make it fast.\n~ \u0026ldquo;The C Language and Models for Systems Programming\u0026rdquo; in Byte magazine (August 1983)\nI wanted to use JWT as a user authentication method. I\u0026rsquo;ve read on many places on the web that it\u0026rsquo;s a bad idea, but still I was devoted to make it work. One of the issues I was aware of was that you cannot easily and permanently logout a user when using JWT.\nIn a nutshell JWT are tokens stored on the client-side. However, due to encryption, they can only be read by the service. So with each request the client sends its token to the sever (like a cookie, you might say). If the token is well-formed, then the server, with quite high certainty, can assume it wasn\u0026rsquo;t tinkered with.\nGoing back to logout issue: to ensure that session will not last indefinitely the server could add \u0026quot;expiryDate\u0026quot; field to payload and check its value with each request and respond accordingly. That works pretty well. The client has no way of modifying \u0026quot;expiryDate\u0026quot;.\nYet it\u0026rsquo;s much harder to kill the session before token expiries. My attempt was to add a new field to token\u0026rsquo;s payload which would indicate that its no longer valid and send it back to the client. The problem though is that the client does not need to use the new token. It still can use the old one and, since we don\u0026rsquo;t store session information on the server, the service has not way of detecting that. 💢\nThe solution I came up with was to use Redis. To store that information on the server-side.\nRedis: The open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker.\nOnce the server decides the user should be logged out, it will store JWT\u0026rsquo;s ID in Redis alongside with an indication of whether the session has ended.\nCan you see now when I said it was overengineered? So many complex solutions for a logout functionality. The project ended because I was too wornout to finish it.\nPlatform 3: Polaris (React) I\u0026rsquo;m actually quite proud of this one. It\u0026rsquo;s a static website running on React and hosting my vector graphics. It\u0026rsquo;s painfully simple, but that was kinda the point. I wanted to have a way of hosting those images ASAP, hence React and GitHub Pages, where the website is actually hosted.\nIt wasn\u0026rsquo;t my first contact with technologies used in frontend, but it was the first time when I used state of the art tools for a new website. My knowledge of NodeJS and utilities built on top of it was practically nonexistent. That changed once I\u0026rsquo;ve written Polaris; now I\u0026rsquo;m just new to this stuff.\nSteam on the horizon I don\u0026rsquo;t have much more to say here other that, it was a while when I\u0026rsquo;ve used Inkscape to create those images and when I needed to use it again, for the sake of this blog, it was terrifying to see how much one can forget what one has learned. ಠ_ಠ\nPlatforms long forgotten There were many more projects which aimed to create my personal space on the web, but only these mentioned above are still remembered by me enough to write a few sentences about.\nIt\u0026rsquo;s safe to say they all suffered from the same fundamental flaws:\nthey were too complex, they were trying to solve all possible future problem without aiming to deliver the most basic functionality, backend is hard. This blog, on the other hand, is a static content website build with Hugo. I think to some extend I was aware that tools like Hugo existed, but I\u0026rsquo;ve never considered using them. I cannot really explain as to why; maybe I was trying too hard to use a new fancy tool I\u0026rsquo;ve just learned about.\nThe goal and future of this project The goal of this project is to create an archive for stuff I\u0026rsquo;m going to learn. It\u0026rsquo;s still unclear as to what I\u0026rsquo;m going to post on this blog, but it\u0026rsquo;s safe to say that it\u0026rsquo;s going to be techy.\nI cannot say with any amount of certainty how often I\u0026rsquo;ll be writing new articles. I\u0026rsquo;m really looking forward to making new content though. I believe it will also tilt me significantly into learning about new things.\nThere are still some adjustments I need to make on the website, I\u0026rsquo;m probably going to focus on them before I\u0026rsquo;ll work on new articles, but in general it is functionally complete.\nSo as of now, thank you for reading.\n🌊\n","permalink":"https://madebyme.today/articles/beginnings-are-hard/","summary":"Well it took some time (almost 6 years) to create a personal blog, but here we are.\n","title":"Beginnings Are Hard"}]